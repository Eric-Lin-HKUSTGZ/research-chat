"""
FastAPI Research Chat Routes
ç ”ç©¶èŠå¤©è·¯ç”±æ¨¡å— - FastAPI å®ç°
è¿ç§»è‡ª Flask Blueprint
"""
from fastapi import APIRouter, Depends, Header, Request, Response, BackgroundTasks
from typing import Optional
from datetime import datetime
from ..utils.tools import UTC8
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session
import time
import logging
import os

from app.services.auth import get_current_user
from app.core.database import get_db, SessionLocal
from app.entity.research_chat import ResearchChatSession, ResearchChatMessage, ResearchChatProcessInfo
from app.utils.logger import get_logger
from app.utils.error_handler import ErrorCode, ErrorResponse, ErrorMessage
from app.services.llm_service import LLMClient, get_newest_paper, get_highly_cited_paper, get_relevence_paper, get_prompt, construct_paper
from app.constants.task_status import CreationStatus
from sqlalchemy import select

logger = get_logger('research_chat_fastapi')

# åˆ›å»º APIRouter
router = APIRouter(prefix="/digital_twin/research_chat/api", tags=["research_chat"])

def get_localized_message(key: str, locale: str = "cn") -> str:
    """æ ¹æ®localeè¿”å›å¯¹åº”è¯­è¨€çš„æ—¥å¿—ä¿¡æ¯"""
    messages = {
        "task_start": {
            "cn": "ğŸš€ ç ”ç©¶ä»»åŠ¡å¯åŠ¨ï¼",
            "en": "ğŸš€ Research task started!"
        },
        "step1_keywords": {
            "cn": "ğŸ”„ ç¬¬ä¸€æ­¥ï¼šæå–ç ”ç©¶å…³é”®è¯...",
            "en": "ğŸ”„ Step 1: Extracting research keywords..."
        },
        "keywords_complete": {
            "cn": "âœ… å…³é”®è¯æå–å®Œæˆï¼",
            "en": "âœ… Keywords extraction completed!"
        },
        "step2_papers": {
            "cn": "ğŸ”„ ç¬¬äºŒæ­¥ï¼šæ£€ç´¢ç›¸å…³è®ºæ–‡...",
            "en": "ğŸ”„ Step 2: Retrieving related papers..."
        },
        "papers_complete": {
            "cn": "âœ… è®ºæ–‡æ£€ç´¢å®Œæˆï¼",
            "en": "âœ… Paper retrieval completed!"
        },
        "step3_inspiration": {
            "cn": "ğŸ”„ ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆç ”ç©¶çµæ„Ÿ...",
            "en": "ğŸ”„ Step 3: Generating research inspiration..."
        },
        "inspiration_complete": {
            "cn": "âœ… ç ”ç©¶çµæ„Ÿç”Ÿæˆå®Œæˆï¼",
            "en": "âœ… Research inspiration generated!"
        },
        "step4_plan": {
            "cn": "ğŸ”„ ç¬¬å››æ­¥ï¼šç”Ÿæˆåˆæ­¥ç ”ç©¶è®¡åˆ’...",
            "en": "ğŸ”„ Step 4: Generating preliminary research plan..."
        },
        "plan_complete": {
            "cn": "âœ… åˆæ­¥ç ”ç©¶è®¡åˆ’ç”Ÿæˆå®Œæˆï¼",
            "en": "âœ… Preliminary research plan generated!"
        },
        "step5_review": {
            "cn": "ğŸ”„ ç¬¬äº”æ­¥ï¼šæ‰¹åˆ¤æ€§å®¡æŸ¥...",
            "en": "ğŸ”„ Step 5: Critical review..."
        },
        "review_complete": {
            "cn": "âœ… æ‰¹åˆ¤æ€§å®¡æŸ¥å®Œæˆï¼",
            "en": "âœ… Critical review completed!"
        },
        "step6_finalize": {
            "cn": "ğŸ”„ ç¬¬å…­æ­¥ï¼šå®Œå–„æœ€ç»ˆç ”ç©¶è®¡åˆ’...",
            "en": "ğŸ”„ Step 6: Finalizing research plan..."
        },
        "finalize_complete": {
            "cn": "âœ… æœ€ç»ˆç ”ç©¶è®¡åˆ’å®Œå–„å®Œæˆï¼",
            "en": "âœ… Final research plan completed!"
        },
        "task_complete": {
            "cn": "ğŸ‰ ç ”ç©¶ä»»åŠ¡å®Œæˆï¼",
            "en": "ğŸ‰ Research task completed!"
        },
        "task_failed": {
            "cn": "âŒ ç ”ç©¶ä»»åŠ¡å¤±è´¥",
            "en": "âŒ Research task failed"
        },
        "llm_init_failed": {
            "cn": "LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥",
            "en": "LLM client initialization failed"
        },
        "keywords_failed": {
            "cn": "å…³é”®è¯æå–å¤±è´¥",
            "en": "Keywords extraction failed"
        },
        "papers_failed": {
            "cn": "è®ºæ–‡æ£€ç´¢å¤±è´¥",
            "en": "Paper retrieval failed"
        },
        "inspiration_failed": {
            "cn": "ç ”ç©¶çµæ„Ÿç”Ÿæˆå¤±è´¥",
            "en": "Research inspiration generation failed"
        },
        "plan_failed": {
            "cn": "ç ”ç©¶è®¡åˆ’ç”Ÿæˆå¤±è´¥",
            "en": "Research plan generation failed"
        },
        "review_failed": {
            "cn": "æ‰¹åˆ¤æ€§å®¡æŸ¥å¤±è´¥",
            "en": "Critical review failed"
        },
        "finalize_failed": {
            "cn": "ç ”ç©¶è®¡åˆ’å®Œå–„å¤±è´¥",
            "en": "Research plan finalization failed"
        }
    }
    
    return messages.get(key, {}).get(locale, messages.get(key, {}).get("cn", key))


def format_log_with_timestamp(message: str) -> str:
    """ä¸ºæ—¥å¿—æ¶ˆæ¯æ·»åŠ æ—¶é—´æˆ³"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    return f"[{timestamp}] {message}"


# ===== Pydantic æ¨¡å‹å®šä¹‰ =====

class CreateResearchRequest(BaseModel):
    content: str = Field(..., description="ç ”ç©¶å†…å®¹/é—®é¢˜")
    session_id: Optional[str] = Field(None, description="ä¼šè¯ID")
    locale: str = Field("cn", description="ç•Œé¢è¯­è¨€ï¼Œcn=ä¸­æ–‡ï¼Œen=è‹±æ–‡")


class UpdateSessionNameRequest(BaseModel):
    session_name: str = Field(..., description="æ–°çš„ä¼šè¯åç§°")


class StandardResponse(BaseModel):
    code: int = 200
    message: str = "Success"
    success: bool = True
    data: dict = {}


# ===== è¾…åŠ©å‡½æ•° =====

def generate_page_session_id(x_page_id: Optional[str] = None) -> str:
    """ç”Ÿæˆ page_session_id"""
    return x_page_id or f"page_{int(time.time() * 1000)}"


# ===== è·¯ç”±å¤„ç†å™¨ =====

@router.get("/sessions")
async def get_sessions(
    page: Optional[int] = None,
    size: Optional[int] = None,
    current_user: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """è·å–ä¼šè¯åˆ—è¡¨"""
    try:
        user_id = current_user["user_id"]

        # æ„å»ºæŸ¥è¯¢
        query = select(ResearchChatSession).where(
            ResearchChatSession.user_id == user_id
        ).order_by(ResearchChatSession.updated_at.desc())

        # å§‹ç»ˆä¿æŒåˆ†é¡µæŸ¥è¯¢
        page = int(page or 1)
        size = int(size or 20)
        size = max(1, min(size, 50))
        items = db.execute(query.offset((page - 1) * size).limit(size)).scalars().all()
        
        # è·å–æ€»æ•°
        from sqlalchemy import func
        total = db.scalar(
            select(func.count(ResearchChatSession.id)).where(
                ResearchChatSession.user_id == user_id
            )
        )
        
        result = {
            "user_id": user_id,
            "chat_type": "research_chat",
            "sessions": [
                {
                    "id": s.id,
                    "session_id": s.page_session_id,
                    "session_name": s.session_name or "",
                    "is_active": s.is_active,
                    "created_at": s.created_at.isoformat() if s.created_at else None,
                    "updated_at": s.updated_at.isoformat() if s.updated_at else None,
                }
                for s in items
            ],
            "pagination": {
                "page": page,
                "size": size,
                "total": total,
                "pages": (total + size - 1) // size if total is not None else 0
            }
        }
        return ErrorResponse.success_response("æˆåŠŸ", result)

    except Exception as e:
        logger.error(f"è·å–ä¼šè¯åˆ—è¡¨å¤±è´¥: {e}")
        return ErrorResponse.create_error_response(ErrorCode.INTERNAL_SERVER_ERROR, ErrorMessage.INTERNAL_SERVER_ERROR)


@router.get("/sessions/{session_id}/messages")
async def get_session_messages(
    session_id: str,
    request: Request,
    current_user: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """è·å–ä¼šè¯æ¶ˆæ¯"""
    try:
        user_id = current_user["user_id"]
        latest = str(request.query_params.get('latest') or '').lower() in {'1', 'true', 'yes'}
        page_param = request.query_params.get('page')
        size_param = request.query_params.get('size')

        def _to_conversations(rows):
            convs = []
            for m, p in rows:
                convs.append({
                    "id": m.id,
                    "question": (m.content or ""),
                    "answer": (m.result_papers or None),
                    "process": {
                        "id": p.id if p else None,
                        "creation_status": p.creation_status if p else None,
                        "process_info": p.process_info if p else None,
                        "created_at": p.created_at.isoformat() if p and p.created_at else None,
                        "updated_at": p.updated_at.isoformat() if p and p.updated_at else None,
                    } if p else {},
                    "question_timestamp": m.created_at.isoformat() if m.created_at else None,
                    "answer_timestamp": m.updated_at.isoformat() if m.updated_at else None,
                })
            return convs

        # éªŒè¯ä¼šè¯æƒé™
        session = db.scalar(
            select(ResearchChatSession).where(
                ResearchChatSession.page_session_id == session_id,
                ResearchChatSession.user_id == user_id
            )
        )

        if not session:
            return ErrorResponse.success_response("æˆåŠŸ", [])

        # æ„å»ºå¤æ‚çš„æŸ¥è¯¢é€»è¾‘ï¼Œä¸lit_research.pyä¿æŒä¸€è‡´
        from sqlalchemy import func
        from sqlalchemy.orm import aliased
        
        # åˆ›å»ºæ’åå­æŸ¥è¯¢
        ranked = select(
            ResearchChatProcessInfo.id.label('pid'),
            ResearchChatProcessInfo.message_id.label('mid'),
            func.row_number().over(
                partition_by=ResearchChatProcessInfo.message_id,
                order_by=ResearchChatProcessInfo.created_at.desc()
            ).label('rn')
        ).where(
            ResearchChatProcessInfo.user_id == user_id
        ).subquery()
        
        # è·å–æœ€æ–°çš„è¿›ç¨‹ä¿¡æ¯
        latest_proc = select(ranked.c.pid, ranked.c.mid).where(ranked.c.rn == 1).subquery()
        
        # åˆ›å»ºè¿›ç¨‹ä¿¡æ¯åˆ«å
        P = aliased(ResearchChatProcessInfo)
        
        # æ„å»ºåŸºç¡€æŸ¥è¯¢è¯­å¥
        base_stmt = select(
            ResearchChatMessage,
            P
        ).select_from(ResearchChatMessage).outerjoin(
            latest_proc, latest_proc.c.mid == ResearchChatMessage.id
        ).outerjoin(
            P, P.id == latest_proc.c.pid
        ).where(
            ResearchChatMessage.session_id == session.id,
            ResearchChatMessage.user_id == user_id
        )

        if latest:
            row = db.execute(base_stmt.order_by(ResearchChatMessage.created_at.desc()).limit(1)).first()
            convs = _to_conversations([row]) if row else []
            return ErrorResponse.success_response("æˆåŠŸ", convs)
        
        if page_param is None and size_param is None:
            rows = db.execute(base_stmt.order_by(ResearchChatMessage.created_at.asc())).all()
            convs = _to_conversations(rows)
            return ErrorResponse.success_response("æˆåŠŸ", convs)
        
        # åˆ†é¡µæŸ¥è¯¢
        page = int(page_param or 1)
        size = int(size_param or 20)
        size = max(1, min(size, 50))
        rows = db.execute(base_stmt.order_by(ResearchChatMessage.created_at.asc()).offset((page - 1) * size).limit(size)).all()
        
        # è·å–æ€»æ•°
        total = db.scalar(
            select(func.count(ResearchChatMessage.id)).where(
                ResearchChatMessage.session_id == session.id,
                ResearchChatMessage.user_id == user_id
            )
        )
        
        result = {
            "content": _to_conversations(rows),
            "pagination": {
                "page": page,
                "size": size,
                "total": total,
                "pages": (total + size - 1) // size if total is not None else 0
            }
        }
        return ErrorResponse.success_response("æˆåŠŸ", result)

    except Exception as e:
        logger.error(f"è·å–ä¼šè¯æ¶ˆæ¯å¤±è´¥: {e}")
        return ErrorResponse.create_error_response(ErrorCode.INTERNAL_SERVER_ERROR, ErrorMessage.INTERNAL_SERVER_ERROR)


@router.put("/sessions/{session_id}/name")
async def update_session_name(
    session_id: str,
    request: UpdateSessionNameRequest,
    current_user: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """æ›´æ–°ä¼šè¯åç§°"""
    try:
        user_id = current_user["user_id"]

        # éªŒè¯ä¼šè¯æƒé™
        session = db.scalar(
            select(ResearchChatSession).where(
                ResearchChatSession.page_session_id == session_id,
                ResearchChatSession.user_id == user_id
            )
        )

        if not session:
            return ErrorResponse.create_error_response(ErrorCode.NOT_FOUND, "ä¼šè¯ä¸å­˜åœ¨")

        # æ›´æ–°ä¼šè¯åç§°
        session.session_name = request.session_name
        db.commit()

        logger.info(f"ä¼šè¯åç§°å·²æ›´æ–°: {session_id} -> {request.session_name}")

        return ErrorResponse.success_response("ä¼šè¯åç§°æ›´æ–°æˆåŠŸ")

    except Exception as e:
        logger.error(f"æ›´æ–°ä¼šè¯åç§°å¤±è´¥: {e}")
        db.rollback()
        return ErrorResponse.create_error_response(ErrorCode.INTERNAL_SERVER_ERROR, ErrorMessage.INTERNAL_SERVER_ERROR)


@router.delete("/sessions/{session_id}")
async def delete_session(
    session_id: str,
    current_user: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """åˆ é™¤ä¼šè¯"""
    try:
        user_id = current_user["user_id"]

        # éªŒè¯ä¼šè¯æƒé™
        session = db.scalar(
            select(ResearchChatSession).where(
                ResearchChatSession.page_session_id == session_id,
                ResearchChatSession.user_id == user_id
            )
        )

        if not session:
            return ErrorResponse.create_error_response(ErrorCode.NOT_FOUND, "ä¼šè¯ä¸å­˜åœ¨")

        # åˆ é™¤ä¼šè¯ï¼ˆçº§è”åˆ é™¤æ¶ˆæ¯å’Œè¿›ç¨‹ä¿¡æ¯ï¼‰
        db.delete(session)
        db.commit()

        logger.info(f"ä¼šè¯å·²åˆ é™¤: {session_id}")

        return ErrorResponse.success_response("ä¼šè¯åˆ é™¤æˆåŠŸ", {"session_id": session_id})

    except Exception as e:
        logger.error(f"åˆ é™¤ä¼šè¯å¤±è´¥: {e}")
        db.rollback()
        return ErrorResponse.create_error_response(ErrorCode.INTERNAL_SERVER_ERROR, ErrorMessage.INTERNAL_SERVER_ERROR)


@router.post("/create")
async def create_research(
    request: CreateResearchRequest,
    response: Response,
    x_page_id: Optional[str] = Header(None),
    background_tasks: BackgroundTasks = None,
    current_user: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    åˆ›å»ºç ”ç©¶è¯·æ±‚ï¼ˆå¼‚æ­¥è§£è€¦ï¼‰
    - æ£€æŸ¥å½“å‰ä¼šè¯æ˜¯å¦æœ‰æ­£åœ¨å¤„ç†ä¸­çš„ä»»åŠ¡ï¼ˆé¿å…åŒä¸€ä¼šè¯å¤šä¸ªWebSocketè¿æ¥ï¼‰
    - ç«‹å³å†™å…¥ messages å’Œ process_infos
    - å¯åŠ¨åå°ä»»åŠ¡å¤„ç† prompt/LLM å¹¶æ›´æ–°æ•°æ®åº“
    - ç«‹å³è¿”å› message_id å’Œ session_idï¼Œå‰ç«¯æ®æ­¤å»ºç«‹ WebSocket
    """
    try:
        user_id = current_user["user_id"]
        user_email = current_user["email"]
        content = request.content
        session_id = request.session_id
        locale = request.locale  # æå–localeå‚æ•°

        # è·å–æˆ–åˆ›å»ºä¼šè¯
        if session_id:
            session = db.scalar(
                select(ResearchChatSession).where(
                    ResearchChatSession.page_session_id == session_id,
                    ResearchChatSession.user_id == user_id
                )
            )
        else:
            session = None

        if not session:
            # åˆ›å»ºæ–°ä¼šè¯
            new_session_id = generate_page_session_id(x_page_id)
            session_name = content[:30] + '...' if len(content) > 30 else content
            session = ResearchChatSession(
                page_session_id=new_session_id,
                user_id=user_id,
                email=user_email,
                session_name=session_name,
                is_active=True
            )
            db.add(session)
            db.flush()

        # æ£€æŸ¥å½“å‰ä¼šè¯æ˜¯å¦æœ‰æ­£åœ¨å¤„ç†ä¸­çš„ä»»åŠ¡
        # creation_status ä¸º 'pending' æˆ– 'creating' æ—¶è¡¨ç¤ºä»»åŠ¡æ­£åœ¨å¤„ç†ä¸­
        in_progress_task = db.scalar(
            select(ResearchChatProcessInfo).where(
                ResearchChatProcessInfo.session_id == session.id,
                ResearchChatProcessInfo.user_id == user_id,
                ResearchChatProcessInfo.creation_status.in_(CreationStatus.IN_PROGRESS)
            )
        )

        if in_progress_task:
            logger.warning(f"ä¼šè¯ {session.page_session_id} å·²æœ‰æ­£åœ¨å¤„ç†ä¸­çš„ä»»åŠ¡ (message_id={in_progress_task.message_id})ï¼Œæ‹’ç»åˆ›å»ºæ–°æ¶ˆæ¯")
            # è®¾ç½® HTTP çŠ¶æ€ç ä¸º 409ï¼ŒåŒæ—¶ä¿æŒ ErrorResponse æ ¼å¼
            response.status_code = ErrorCode.CONFLICT.value
            return ErrorResponse.create_error_response(
                ErrorCode.CONFLICT,
                f"å½“å‰ä¼šè¯å·²æœ‰æ­£åœ¨å¤„ç†ä¸­çš„ä»»åŠ¡ï¼Œè¯·ç­‰å¾…ä»»åŠ¡å®Œæˆåå†æäº¤æ–°çš„ç ”ç©¶è¯·æ±‚"
            )

        # åˆ›å»ºæ¶ˆæ¯è®°å½•
        message = ResearchChatMessage(
            session_id=session.id,
            user_id=user_id,
            email=user_email,
            content=content
        )
        db.add(message)
        db.flush()

        # åˆ›å»ºè¿›ç¨‹è®°å½•ï¼ˆåˆå§‹ä¸º creatingï¼‰
        process = ResearchChatProcessInfo(
            session_id=session.id,
            message_id=message.id,
            user_id=user_id,
            email=user_email,
            creation_status=CreationStatus.CREATING,
            process_info={"logs": [format_log_with_timestamp("ğŸš€ å¼€å§‹å¤„ç†ç ”ç©¶è¯·æ±‚")]}
        )
        db.add(process)
        db.commit()

        logger.info(f"ç ”ç©¶è¯·æ±‚å·²åˆ›å»º(å¼‚æ­¥): message_id={message.id}, session={session.page_session_id}")

        # å¯åŠ¨åå°ä»»åŠ¡ï¼ˆå¼‚æ­¥å¤„ç† LLM & æ›´æ–°DBï¼‰
        if background_tasks is not None:
            background_tasks.add_task(_background_process_prompt_and_update,
                                      message.id, session.id, user_id, user_email, content, locale)

        # ç«‹å³è¿”å›ï¼ˆå‰ç«¯æ‹¿åˆ° message_id åå†è¿æ¥ WSï¼‰
        return ErrorResponse.success_response("ç ”ç©¶æ¶ˆæ¯å·²æˆåŠŸåˆ›å»º", {
            "message_id": message.id,
            "session_id": session.page_session_id
        })

    except Exception as e:
        logger.error(f"åˆ›å»ºç ”ç©¶è¯·æ±‚å¤±è´¥: {e}")
        db.rollback()
        return ErrorResponse.create_error_response(ErrorCode.INTERNAL_SERVER_ERROR, ErrorMessage.INTERNAL_SERVER_ERROR)


def _background_process_prompt_and_update(message_id: int, session_db_id: int, user_id: int, user_email: str, content: str, locale: str = "cn"):
    """
    åå°ä»»åŠ¡ï¼šç»“åˆ prompt è°ƒç”¨æ¨¡å‹å¹¶å¼‚æ­¥æ›´æ–°æ•°æ®åº“
    - ä»¿ç…§deepresearchçš„run_research_asyncå‡½æ•°æ¨¡å¼
    - è¿½åŠ è¿›åº¦æ—¥å¿—åˆ° ResearchChatProcessInfo.process_info.logs
    - è°ƒç”¨keyu-ideationçš„6æ­¥ç ”ç©¶è®¡åˆ’ç”Ÿæˆæµç¨‹
    - æ›´æ–° ResearchChatMessage.result_papers ä¸è¿›ç¨‹çŠ¶æ€
    """
    # å®šä¹‰ä»»åŠ¡è¶…æ—¶æ—¶é—´ (1å°æ—¶ = 3600ç§’)
    TASK_TIMEOUT_SECONDS = 3600.0
    
    db = SessionLocal()
    
    # Create task-specific logger (following deepresearch pattern)
    task_logger = logging.getLogger(f"research_task.msg_{message_id}")
    task_logger.setLevel(logging.INFO)
    if task_logger.hasHandlers():
        task_logger.handlers.clear()

    formatter = logging.Formatter(
        f'[%(asctime)s] [%(levelname)s] [msg:{message_id}] %(message)s',
        '%Y-%m-%d %H:%M:%S'
    )

    # File handler
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, f"message_{message_id}_task.log")
    file_handler = logging.FileHandler(log_file, encoding="utf-8")
    file_handler.setFormatter(formatter)
    task_logger.addHandler(file_handler)

    # Stream handler
    stream_handler = logging.StreamHandler()
    stream_handler.setFormatter(formatter)
    task_logger.addHandler(stream_handler)
    
    # æ ¸å¿ƒä¸šåŠ¡é€»è¾‘å‡½æ•°
    def _execute_research():
        # ç”¨æˆ·çŠ¶æ€æ—¥å¿— (db_log) çš„è®¾ç½® - ä»¿ç…§deepresearchæ¨¡å¼
        logs = []
        
        def db_log(msg: str, stage: str = CreationStatus.CREATING):
            nonlocal logs
            log_entry = f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}"
            logs.append(log_entry)
            print(log_entry)  # ä¿ç•™ç®€å•çš„æ§åˆ¶å°è¾“å‡º

            # ç›´æ¥æ›´æ–°æ•°æ®åº“ - ä»¿ç…§deepresearchçš„æ•°æ®åº“æ›´æ–°æ¨¡å¼
            try:
                proc = db.scalar(
                    select(ResearchChatProcessInfo).where(ResearchChatProcessInfo.message_id == message_id)
                )
                if proc:
                    proc.process_info = {"logs": logs}
                    proc.creation_status = stage
                    proc.updated_at = datetime.now()
                    db.commit()
            except Exception as e:
                task_logger.error(f"Failed to update process info: {e}")
                db.rollback()

        try:
            db_log(get_localized_message("task_start", locale))
            task_logger.info(f"Research task started. Topic: '{content}'")
            
            # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
            try:
                client = LLMClient(provider="custom")
                task_logger.info("LLM client initialized successfully")
            except Exception as e:
                raise Exception(get_localized_message("llm_init_failed", locale) + f": {e}")

            # === Step 1: Extract Keywords ===
            db_log(get_localized_message("step1_keywords", locale))
            task_logger.info("Step 1: Extracting keywords from query")
            try:
                prompt = get_prompt("retrieve_query", locale=locale, user_query=content)
                response = client.get_response(prompt=prompt)
                task_logger.info(f"Keywords extracted: {response}")
                
                query_list = [kw.strip() for kw in response.split(",")]
                if len(query_list) == 1:
                    query = query_list[0]
                else:
                    query = " | ".join(f'"{item}"' for item in query_list)
                
                db_log(get_localized_message("keywords_complete", locale))
                task_logger.info(f"Constructed query: {query}")
            except Exception as e:
                raise Exception(get_localized_message("keywords_failed", locale) + f": {e}")

            # === Step 2: Retrieve Papers ===
            db_log(get_localized_message("step2_papers", locale))
            task_logger.info("Step 2: Retrieving related papers")
            try:
                newest_paper = get_newest_paper(query)
                highly_cited_paper = get_highly_cited_paper(query)
                relevence_paper = get_relevence_paper(query)
                paper = construct_paper(newest_paper, highly_cited_paper, relevence_paper)
                
                task_logger.info(f"Papers retrieved: {len(newest_paper)} newest, {len(highly_cited_paper)} highly cited, {len(relevence_paper)} relevant")
                db_log(get_localized_message("papers_complete", locale))
            except Exception as e:
                raise Exception(get_localized_message("papers_failed", locale) + f": {e}")

            # === Step 3: Generate Inspiration ===
            db_log(get_localized_message("step3_inspiration", locale))
            task_logger.info("Step 3: Generating inspiration from papers")
            try:
                prompt = get_prompt("get_inspiration", locale=locale, user_query=content, paper=paper)
                inspiration = client.get_response(prompt=prompt)
                task_logger.info(f"Inspiration generated (length: {len(inspiration)} chars)")
                db_log(get_localized_message("inspiration_complete", locale))
            except Exception as e:
                raise Exception(get_localized_message("inspiration_failed", locale) + f": {e}")

            # === Step 4: Generate Preliminary Plan ===
            db_log(get_localized_message("step4_plan", locale))
            task_logger.info("Step 4: Generating preliminary research plan")
            try:
                prompt = get_prompt("generate_research_plan", locale=locale, user_query=content, paper=paper, inspiration=inspiration)
                research_plan = client.get_response(prompt=prompt)
                task_logger.info(f"Preliminary plan generated (length: {len(research_plan)} chars)")
                db_log(get_localized_message("plan_complete", locale))
            except Exception as e:
                raise Exception(get_localized_message("plan_failed", locale) + f": {e}")

            # === Step 5: Critical Review ===
            db_log(get_localized_message("step5_review", locale))
            task_logger.info("Step 5: Conducting critical review")
            try:
                prompt = get_prompt("critic_research_plan", locale=locale, user_query=content, paper=paper, inspiration=inspiration, research_plan=research_plan)
                criticism = client.get_response(prompt=prompt)
                task_logger.info(f"Critical review completed (length: {len(criticism)} chars)")
                db_log(get_localized_message("review_complete", locale))
            except Exception as e:
                raise Exception(get_localized_message("review_failed", locale) + f": {e}")

            # === Step 6: Refine Plan ===
            db_log(get_localized_message("step6_finalize", locale))
            task_logger.info("Step 6: Refining research plan based on criticism")
            try:
                prompt = get_prompt("refine_research_plan", locale=locale, user_query=content, research_plan=research_plan, criticism=criticism)
                final_research_plan = client.get_response(prompt=prompt)
                task_logger.info(f"Final plan generated (length: {len(final_research_plan)} chars)")
                db_log(get_localized_message("finalize_complete", locale))
            except Exception as e:
                raise Exception(get_localized_message("finalize_failed", locale) + f": {e}")

            # === Update Message with Final Result ===
            db_log("ğŸ”„ ä¿å­˜æœ€ç»ˆç ”ç©¶è®¡åˆ’...")
            task_logger.info("Saving final research plan to database")
            try:
                msg = db.scalar(select(ResearchChatMessage).where(ResearchChatMessage.id == message_id))
                if msg:
                    result_data_to_save = {
                        "response": final_research_plan,
            "generated_at": datetime.now().isoformat(),
                        "mode": "background-llm",
                        "intermediate_results": {
                            "keywords": response,
                            "query": query,
                            "papers_count": {
                                "newest": len(newest_paper),
                                "highly_cited": len(highly_cited_paper),
                                "relevant": len(relevence_paper)
                            },
                            "inspiration": inspiration,
                            "preliminary_plan": research_plan,
                            "criticism": criticism
                        }
                    }
                    msg.result_papers = result_data_to_save
                    msg.updated_at = datetime.now()
                    msg.extra_info = {"generation_complete": True}
                    db.commit()
                    task_logger.info("Final research plan saved to database successfully")
            except Exception as e:
                raise Exception(f"ä¿å­˜æœ€ç»ˆç ”ç©¶è®¡åˆ’å¤±è´¥: {e}")

            db_log(get_localized_message("task_complete", locale), stage=CreationStatus.CREATED)
            task_logger.info("===== TASK COMPLETED SUCCESSFULLY =====")

        except Exception as e:
            # å¯¹ç”¨æˆ·ï¼Œåªè®°å½•ç®€æ´çš„é”™è¯¯ä¿¡æ¯
            error_message_for_user = get_localized_message("task_failed", locale) + f": {str(e)}"
            db_log(error_message_for_user, stage=CreationStatus.FAILED)
            
            # å¯¹å¼€å‘è€…ï¼Œä½¿ç”¨ exception è®°å½•å®Œæ•´çš„å †æ ˆè·Ÿè¸ªåˆ°æ—¥å¿—æ–‡ä»¶
            task_logger.exception("A critical error caused the research task to fail. See traceback below.")
            task_logger.error("===== TASK FAILED =====")

    # ä¸»è¦æ‰§è¡Œé€»è¾‘ - ä»¿ç…§deepresearchçš„è¶…æ—¶å¤„ç†
    try:
        task_logger.info(f"Task starting with a timeout of {TASK_TIMEOUT_SECONDS} seconds.")
        
        # ä½¿ç”¨threading.Timeræ¥æ¨¡æ‹Ÿè¶…æ—¶å¤„ç†ï¼ˆå› ä¸ºè¿™æ˜¯åŒæ­¥å‡½æ•°ï¼‰
        import threading
        import signal
        
        def timeout_handler():
            raise TimeoutError(f"Task timed out after {TASK_TIMEOUT_SECONDS} seconds")
        
        # è®¾ç½®è¶…æ—¶å®šæ—¶å™¨
        timer = threading.Timer(TASK_TIMEOUT_SECONDS, timeout_handler)
        timer.start()
        
        try:
            _execute_research()
        finally:
            timer.cancel()
            
    except TimeoutError as e:
        # è¶…æ—¶å¤„ç† - ä»¿ç…§deepresearchçš„è¶…æ—¶å¤„ç†æ¨¡å¼
        timeout_message = "âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: è¿è¡Œè¶…è¿‡1å°æ—¶ï¼Œå·²è‡ªåŠ¨è¶…æ—¶ã€‚"
        task_logger.error(f"Task timed out after {TASK_TIMEOUT_SECONDS} seconds. Marking as failed.")
        
        # ç›´æ¥æ›´æ–°æ•°æ®åº“çŠ¶æ€ä¸º failed
        try:
            proc = db.scalar(
                select(ResearchChatProcessInfo).where(ResearchChatProcessInfo.message_id == message_id)
            )
            if proc and proc.process_info and 'logs' in proc.process_info:
                logs_on_timeout = proc.process_info['logs']
            else:
                logs_on_timeout = []
            
            log_entry = f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {timeout_message}"
            logs_on_timeout.append(log_entry)
            
            proc.process_info = {"logs": logs_on_timeout}
            proc.creation_status = CreationStatus.FAILED
            proc.updated_at = datetime.now()
            db.commit()
            
            task_logger.error("===== TASK FAILED DUE TO TIMEOUT =====")
        except Exception as e:
            task_logger.error(f"Failed to update timeout status: {e}")
            db.rollback()
        
    finally:
        # ä»»åŠ¡ç»“æŸæ—¶ (æ— è®ºæˆåŠŸã€å¤±è´¥è¿˜æ˜¯è¶…æ—¶)ï¼Œéƒ½å…³é—­å¹¶ç§»é™¤handler
        for handler in task_logger.handlers[:]:
            handler.close()
            task_logger.removeHandler(handler)
        db.close()

@router.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return ErrorResponse.success_response("å¥åº·æ£€æŸ¥é€šè¿‡", {
        "status": "healthy",
        "timestamp": datetime.now(UTC8).isoformat()
    })
